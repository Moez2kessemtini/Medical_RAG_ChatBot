# rebuild_vectorstore.py
"""
Script pour reconstruire compl√®tement le vectorstore FAISS
avec les m√©tadonn√©es correctes.
"""
import os
import shutil
import logging
from pathlib import Path
from typing import List, Dict, Any

import pdfplumber
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Param√®tres
PDF_PATH = Path("../data/Medical_book.pdf")
VECTOR_DIR = Path("faiss_index")
EMBED_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"

def load_and_split_pdf(pdf_path: Path) -> List[Dict[str, Any]]:
    """Charge et d√©coupe le PDF en chunks avec m√©tadonn√©es."""
    logger.info(f"üìñ Chargement du PDF: {pdf_path}")
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF introuvable: {pdf_path}")
    
    pages_text = []
    
    with pdfplumber.open(pdf_path) as pdf:
        logger.info(f"üìÑ Nombre total de pages: {len(pdf.pages)}")
        
        for i, page in enumerate(pdf.pages):
            txt = page.extract_text() or ""
            page_num = i + 1
            
            # Garde les pages avec minimum 50 caract√®res
            if len(txt.strip()) > 50:
                pages_text.append((page_num, txt))
    
    logger.info(f"‚úÖ {len(pages_text)} pages conserv√©es")
    
    # D√©coupage en chunks
    chunks = []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    logger.info("‚úÇÔ∏è  D√©coupage en chunks...")
    
    for p_no, txt in pages_text:
        raw_chunks = splitter.split_text(txt)
        
        for idx, c in enumerate(raw_chunks):
            cleaned = c.strip()
            if len(cleaned) > 20:  # Minimum 20 caract√®res
                chunks.append({
                    "text": cleaned,
                    "page": p_no,
                    "chunk_id": f"p{p_no}_c{idx}"
                })
    
    logger.info(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    
    # Affiche statistiques
    pages_with_chunks = set(c["page"] for c in chunks)
    logger.info(f"üìä Pages avec contenu: {len(pages_with_chunks)}")
    
    # Affiche exemples de chunks m√©dical (pas juste titre)
    medical_chunks = [c for c in chunks if len(c["text"]) > 100 and c["page"] > 10]
    if medical_chunks:
        logger.info(f"\nüìù Exemples de chunks m√©dicaux:")
        for i, chunk in enumerate(medical_chunks[:3], 1):
            logger.info(f"\n{i}. Page {chunk['page']} ({chunk['chunk_id']}):")
            logger.info(f"   {chunk['text'][:200]}...")
    
    return chunks

def create_vectorstore(chunks: List[Dict[str, Any]], vector_dir: Path):
    """Cr√©e un nouveau vectorstore FAISS."""
    if not chunks:
        raise ValueError("Aucun chunk √† indexer!")
    
    logger.info(f"üîÆ Cr√©ation du vectorstore FAISS...")
    logger.info(f"   - Mod√®le: {EMBED_MODEL_NAME}")
    logger.info(f"   - Nombre de chunks: {len(chunks)}")
    
    # Pr√©pare les textes et m√©tadonn√©es
    texts = [c["text"] for c in chunks]
    metadatas = [
        {
            "page": c["page"],
            "chunk_id": c["chunk_id"],
            "text_length": len(c["text"])
        } 
        for c in chunks
    ]
    
    # V√©rifie que les m√©tadonn√©es sont bien pr√©sentes
    logger.info(f"üìã Exemple de m√©tadonn√©es:")
    for i in range(min(3, len(metadatas))):
        logger.info(f"   {i+1}. {metadatas[i]}")
    
    # Cr√©e l'embedding function
    embedding_fn = SentenceTransformerEmbeddings(model_name=EMBED_MODEL_NAME)
    
    # Cr√©e le vectorstore
    logger.info("‚è≥ Cr√©ation des embeddings (cela peut prendre quelques minutes)...")
    vectordb = FAISS.from_texts(
        texts=texts,
        embedding=embedding_fn,
        metadatas=metadatas
    )
    
    # Sauvegarde
    if vector_dir.exists():
        logger.warning(f"‚ö†Ô∏è  Suppression de l'ancien index: {vector_dir}")
        shutil.rmtree(vector_dir)
    
    vector_dir.mkdir(parents=True, exist_ok=True)
    vectordb.save_local(str(vector_dir))
    
    logger.info(f"‚úÖ Vectorstore sauvegard√©: {vector_dir.absolute()}")
    
    return vectordb

def test_vectorstore(vectordb: FAISS):
    """Test le vectorstore avec plusieurs requ√™tes."""
    logger.info(f"\n{'='*70}")
    logger.info(f"üß™ TEST DU VECTORSTORE")
    logger.info(f"{'='*70}\n")
    
    test_queries = [
        "Quels sont les sympt√¥mes du diab√®te ?",
        "Comment traiter l'hypertension ?",
        "Qu'est-ce que l'asthme ?",
        "Sympt√¥mes de la grippe",
        "Traitement du cancer"
    ]
    
    for query in test_queries:
        logger.info(f"\nüîç Requ√™te: '{query}'")
        
        results = vectordb.similarity_search_with_score(query, k=10)
        logger.info(f"   R√©sultats trouv√©s: {len(results)}")
        
        if results:
            logger.info(f"   Top 3:")
            for i, (doc, score) in enumerate(results[:3], 1):
                metadata = doc.metadata
                page = metadata.get("page", "N/A")
                chunk_id = metadata.get("chunk_id", "N/A")
                text_preview = doc.page_content[:100].replace('\n', ' ')
                
                logger.info(f"   {i}. Score: {score:.4f} | Page: {page} | ID: {chunk_id}")
                logger.info(f"      Texte: {text_preview}...")
        else:
            logger.warning(f"   ‚ö†Ô∏è  Aucun r√©sultat trouv√©!")
        
        logger.info("")

def main():
    logger.info(f"\n{'#'*70}")
    logger.info(f"# üîß RECONSTRUCTION DU VECTORSTORE")
    logger.info(f"{'#'*70}\n")
    
    try:
        # 1. Charge le PDF
        chunks = load_and_split_pdf(PDF_PATH)
        
        if not chunks:
            logger.error("‚ùå Aucun chunk cr√©√©. V√©rifiez le PDF.")
            return
        
        # 2. Cr√©e le vectorstore
        vectordb = create_vectorstore(chunks, VECTOR_DIR)
        
        # 3. Test
        test_vectorstore(vectordb)
        
        logger.info(f"\n{'#'*70}")
        logger.info(f"# ‚úÖ RECONSTRUCTION TERMIN√âE")
        logger.info(f"{'#'*70}\n")
        logger.info(f"üëâ Le vectorstore est pr√™t dans: {VECTOR_DIR.absolute()}")
        logger.info(f"üëâ Vous pouvez maintenant relancer rag_pipeline.py")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}", exc_info=True)

if __name__ == "__main__":
    main()